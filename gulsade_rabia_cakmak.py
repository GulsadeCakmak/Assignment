# -*- coding: utf-8 -*-
"""Gulsade Rabia Cakmak.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14otjVy3M7Of_J533sbFlA-vRkPwouZGc
"""

# Gulsade Rabia Cakmak 

import numpy as np
import matplotlib.pyplot as plt


dataRaw_x= np.arange(1000)  # Data is generated by using np.arange() function
dataRaw_y= np.arange(1000)
index=np.arange(1000)
mu=0
sigma=0.15 #Mu and sigma values are specified

GaussianNoise_x = np.random.normal(mu,sigma,len(dataRaw_x)) #Two different gaussian noise is produced and they are added to the raw data array
GaussianNoise_y = np.random.normal(mu,sigma,len(dataRaw_y))

data_x= np.add(dataRaw_x,GaussianNoise_x, dtype='float32')
data_y= np.add(dataRaw_y,GaussianNoise_y, dtype='float32')

plt.figure(figsize=(10, 20))   #This part is to visualize the dataset and the added noise
plt.subplot(411)
plt.plot(data_x, color='r', label='Data-X')
plt.legend()
plt.subplot(412)
plt.plot(data_y, color='b', label='Data-Y')
plt.legend()
plt.subplot(413)
plt.plot(GaussianNoise_x, color='g', label='GaussianNoise_x')
plt.legend()
plt.subplot(414)
plt.plot(GaussianNoise_y, color='k', label='GaussianNoise_y')
plt.legend()
plt.show()

import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

data_x = torch.from_numpy(data_x/np.max(data_x))  #In order to create a pytorch model, the np.ndarray is converted to tensor. Here, the normalization is done, due to large values. It is quite hard to train an algorithm with those large values.
data_y = torch.from_numpy(data_y/np.max(data_y))
dataset = TensorDataset(data_x, data_y)  #The tensor dataset which contains both x and y values in the tuple format 
trainSize = int(0.8 * len(dataset)) #The train:test ratio is determined as 4:1
testSize = len(dataset) - trainSize
trainSet, testSet = torch.utils.data.random_split(dataset, [trainSize, testSize]) #Random splitting of the dataset is done by using random_split() method

from torch.utils.data import DataLoader

loaded_TrainSet = DataLoader(trainSet, shuffle=True) #The dataset (subset) needs to be converted into DataLoader type
loaded_TestSet = DataLoader(testSet, shuffle=True)

model=torch.nn.Linear(1,1) #Model consists of a linear regression method. Since the dataset was not complex, I preferred to create 1 Layer-Model
lossFunction = torch.nn.functional.mse_loss #Mean Squared Error is used as cost function
optimizer = torch.optim.Adam(model.parameters()) #Adam optimizer is preferred

weigth_array=np.empty(80000) #since 800 element is iterator over 100 epoch, the number of produced weight/bias will be 800*100
bias_array=np.empty(80000)
c=0;
for epoch in range(100):  #epoch number is determined as 100
  for x,y in loaded_TrainSet: 

    predictedValue = model(x) #The model is trained by this algorithm
    lossValue = lossFunction(predictedValue, y) #loss value is calculated by referring to the MSE
    lossValue.backward() #backward() function computes the derivative of the loss value. The gradient is formed
    optimizer.step() #The update on the parameters by gradient is done 
    optimizer.zero_grad() #The gradient is cleared for the parameters
    print('epoch {}, loss {}'.format(epoch, lossValue.item()))
    param = model.state_dict() 
    weigth_array[c]=param['weight'] #The weigth and bias are appended inside an array. It is done for the visualization process below.
    bias_array[c]=param['bias']
    c+=1

result_Array_x=np.empty(200)
result_Array_y=np.empty(200)
c=0
lossTest=0
for x,y in loaded_TestSet: #The model is tested on the test set, which contains 200 elements

    result= model(x)
    result_Array_x[c]=result  #The output of the algorithm, real y value are appended to be used below.
    result_Array_y[c]=y
    lossTest += lossFunction(result, y) #The loss value for every element is obtained to calculate the average loss value below.
    c+=1

plt.figure(figsize=(10,10))
plt.plot(result_Array_x, color='r', label='Model_Output', linewidth=2.0)
plt.plot(result_Array_y,color='b', label='Ground_Truth')
plt.legend()
plt.show()

print("The loss value of model on Test Set :",  lossTest/200 ) #Here, the average loss value for the test set is calculated.

plt.figure(figsize=(10,10))  #Weight and Bias are visualized.
plt.plot(weigth_array, linewidth=1.2, label='weight')
plt.legend()
plt.figure(figsize=(10,10))
plt.plot(bias_array, linewidth=1.2, label='bias')
plt.legend()

print(weigth_array[:])
print(bias_array[:])

#the results for weight and bias:

print(weigth_array[-1])
print(bias_array[-1])

Model_output=weigth_array[-1]*data_x + bias_array[-1]  #Here the y values are calculated by using the final weight and the bias of the model.


plt.figure(figsize=(10,10))
plt.subplot(211)
plt.scatter(data_x, data_y, color='r', label='Real Data') #The real data values and model outputs are compared below.
plt.plot(data_x,Model_output, color='b', label='Model Result' )
plt.legend()
plt.subplot(212)
plt.scatter(data_x, data_y, color='r', label='Real Data')
plt.plot(data_x,Model_output, color='b', label='Model Result' )
plt.axis([0.2,0.3,0.2,0.3]) #This is done to zoom in the graph.
plt.legend()
plt.show()

## Linear Regression By using Numpy Library

x_data= np.add(dataRaw_x,GaussianNoise_x, dtype='float32')
y_data= np.add(dataRaw_y,GaussianNoise_y, dtype='float32')

x_data_rescaled=x_data/np.max(dataRaw_x)  #The dataset is normalized, so that the training process can be effective.
y_data_rescaled=y_data/np.max(dataRaw_y)


def MSE_value (bias_new ,weight_new, x_new, y_new):  #The mean squared error is written as a function, corresponding to its mathematical operation. As a result, mean square error of the 
  predicted_y= x_new*weight_new + bias_new           # given data can be calculated.
  squared_error=pow((predicted_y-y_new), 2)
  mean_squared_error=sum(squared_error)/len(x_new)
  return mean_squared_error
  
def Gradient_value(bias_new ,weight_new, x_new, y_new):  #The gradient which is used in the backpropagation and updating the weight and bias
  predicted_y= x_new*(weight_new) + bias_new             #These are created by using the formulas on the literature
  weight_derivative = -(2/len(x_new))*sum(x_new*(y_new-predicted_y))
  bias_derivative = -(2/len(x_new))*sum(y_new-predicted_y)
  return weight_derivative, bias_derivative

init_weight=0.1 #initial weight and bias is given here.
init_bias=0.01
lr=0.01  #The learning rate is adjusted to 0.01 after experiments with much smaller learning rate values.
cost_value=0

loss_stock=[]  #The loss, weights and the bias values are stored for visualization process below.
weight_stock=[]
bias_stock=[]
for epoch in range(5000): #Here epoch is adjusted to 2000, it is done due to ineffective training in small epoch values
  
    
  
  cost_value=MSE_value(init_bias,init_weight, x_data_rescaled[:800], y_data_rescaled[:800]) #Cost value is calculated, by using the written function above.
  weight_derivative, bias_derivative= Gradient_value(init_bias,init_weight, x_data_rescaled[:800], y_data_rescaled[:800]) #The derivatives are calculated by using the written function above.
  init_weight = init_weight - (lr * weight_derivative) #The weight and bias are updated by using the formulas on the literature
  init_bias = init_bias - (lr * bias_derivative)
    
  
  loss_stock.append(cost_value)
  weight_stock.append(init_weight)
  bias_stock.append(init_bias)

  print('epoch {}, loss {}'.format(epoch, cost_value))

numpy_output=weight_stock[-1]*x_data_rescaled[800:] + bias_stock[-1]  #This is done to test the model coefficients on the test set.


plt.figure(figsize=(10,10))
plt.subplot(211)
plt.scatter(x_data_rescaled[800:], y_data_rescaled[800:], color='r', label='Real Data')
plt.plot(x_data_rescaled[800:],numpy_output, color='b', label='Model Result' )
plt.legend()
plt.subplot(212)
plt.scatter(x_data_rescaled[800:], y_data_rescaled[800:], color='r', label='Real Data')
plt.plot(x_data_rescaled[800:],numpy_output, color='b', label='Model Result' )
plt.axis([0.85,0.9,0.85,0.9]) #This is for zoom in the graph
plt.legend()
plt.show()

plt.plot(loss_stock, label='cost value')
plt.legend()

plt.plot(weight_stock, label='weight')
plt.legend()

plt.plot(bias_stock, label='bias')
plt.legend()

